---
title: "Introduction to bayesian modelling with stan"
author: Sylvain SCHMITT
date: "March 1, 2018, ECOFOG"
output: 
  revealjs::revealjs_presentation:
    theme: blood
    highlight: pygments
    center: true
    fig_caption: true
    self_contained: false
    reveal_plugins: ["chalkboard"]
    reveal_options:
      slideNumber: true
      previewLinks: true
  # pdf_document: default
  # html_document:
  #   theme: readable
  #   toc: true
  #   toc_float: yes
---

```{r setup, include=FALSE}
library(knitr)
library(raster)
library(leaflet)
library(tidyverse)
library(rstan)
library(bayesplot)
library(shinystan)
library(rstanarm)
library(brms)
library(greta)
opts_chunk$set(echo = T, eval = T, cache = T,
               fig.height = 5)
```

# Introduction

## Why Bayes

* Express your **beliefs/expertise** about parameters
* Properly account for **uncertainty**
* Handle **small data**
* **Any form** of model

## Bayes theorem

$$p(\theta|y) = \frac{p(\theta)*p(y|\theta)}{p(y)}$$

* $p(\theta)$ represents what someone **believes** about $\theta$ **prior** to observing $y$
* $p(\theta|y)$ represents what someone **believes** about $\theta$ **after** observing $y$
* $p(y|\theta)$ is the **likelihood** function
* $p(y)$ is the **marginal likelihood** equal to $\int p(y|\theta)*p(\theta)*d\theta$

## What is `stan`

```{r, echo=F}
rbind(
c("C++ Math/Stats Library", "Mathematical specification of models"),
c("Imperative Model Specification Language", "Fast and simple way to specify complex models"),
c("Algorithm Toolbox", "Fit with full Bayes, approximate Bayes,
optimization (HMC NUTS, ADVI, L-BFGS)"),
c("Interfaces (Command Line, R, Python, Julia, Matlab, Stata, ...)", "Work in the language of your choice"),
c("Interpretation Tools", "Model critisism, algorithm evaluation")
) %>% 
  data.frame() %>% 
  rename(What = X1, Why = X2) %>% 
  kable()
```

## Outlines

1. `stan` & friends
2. Coin example
3. Leaf lifespan example
4. Growth example
5. Gaps & growth example
6. What about `greta`

# `stan` & friends

## `stan` program

```{stan output.var="", eval=F, cache=F}
data {                      // Data block
  int<lower=1> N;           // Sample size
  int<lower=1> K;           // Dimension of model matrix
  matrix[N, K] X;           // Model Matrix
  vector[N] y;              // Target variable
}
transformed data {          // Transformed data block.
} 
parameters {                // Parameters block
  vector[K] beta;           // Coefficient vector
  real<lower=0> sigma;      // Error scale
}
transformed parameters {    // Transformed parameters block.
} 
model {                     // Model block
  vector[N] mu;
  mu = X * beta;            // Creation of linear predictor
  beta ~ normal(0, 10);     // priors
  sigma ~ cauchy(0, 5);     
  y ~ normal(mu, sigma);    // likelihood
}
generated quantities {      // Generated quantities block. 
}
```

## Data in `stan`

```{stan output.var="", eval=F, cache=F}
data {                       // Data block
  int<lower=1> N ;           // Sample size
  int<lower=1> K ;           // Dimension of model matrix
  matrix[N, K] X ;           // Model Matrix
  vector[N] y ;              // Target variable
}
```

* Define `type<bounds>[size] name`
* Declare first what you need after (e.g. N,K)
* Comment

## Transformed data in `stan`

```{stan output.var="", eval=F, cache=F}
transformed data {          // Transformed data block
  vector[N] logX ;
  logX = log(X) ;
}
```

* log, center, scale, new data...
* Can be done in R
* Can be done directly in model block (if you are not interested with values)

## Parameters in `stan`

```{stan output.var="", eval=F, cache=F}
parameters {                 // Parameters block
  vector[K] beta ;           // Coefficient vector
  real<lower=0> sigma ;      // Error scale
}
```

* Declaration only
* Prefer splitting $\beta$
* Prior boundaries
* Prior relations `real<lower=-beta> alpha`

## Transformed parameters in `stan`

```{stan output.var="", eval=F, cache=F}
transformed parameters {    // Transformed parameters block
  vector[N] mu ;                
  mu = X * beta ;           // Creation of linear predictor
} 
```

* Can be done directly in the model block (if you are not interested with values)

## Model in `stan`

```{stan output.var="", eval=F, cache=F}
model {                      // Model block
  beta ~ normal(0, 10) ;     // priors
  sigma ~ cauchy(0, 5) ;     
  y ~ normal(mu, sigma) ;    // likelihood
  log(y) ~ normal(X * beta, sigma) ;
}
```

* Priors definition
* Likelihood defintion
* Directly define data and parameter transformation here (if you are not interested with values)

## Generated quantities in `stan`

```{stan output.var="", eval=F, cache=F}
generated quantities {
  vector[N] yhat ;                // linear predictor
  yhat = X * beta ;
}
```

* Anything you want to extract/calculate can be defined here
* Predicted values (yhat), Residual Sum of Squares (RSS), total RSS, $R^2$...

## `rstan`

> `rstan` is the R interface to the `stan` C++ package

* model compilation
* model sampling
* sampling extraction

## `bayesplot`

> `bayesplot` provides `ggplot2`-based plotting functions for Bayesian model fits

* **MCMC:** Visualizations of Markov chain Monte Carlo (MCMC)
    * chain traces, parameter pairs, parameter posteriors, combos...
* **PPC:** Graphical posterior predictive checks (PPCs).
* see `broom::tidy_mcmc` for tables

## `shinystan`

> `ShinyStan` provide a graphical user interface (GUI) to analyze `rstan` outputs interactivelly 

```{r, echo=F}
include_graphics("./stan_data/shinystan.png")
```

## `rstanarm`

> `rstanarm` allows models specification using customary R modeling syntax

* Use formula and data frames
* E.g. `stan_lm(y ~ x, data)`
* Support `lm`, `aov`, `glm`, `glmer`, `gamm4`, `polr`, `betareg`, `clogit`

## `brms`

> Bayesian generalized (non-)linear multilevel models using `stan` with extended version of the syntax applied in `lme4`

* Use formula and data frames

```{r, eval=F}
brm(formula = time | cens(censored) ~ age * sex + disease
    + (1 + age|patient),
    data = kidney, family = lognormal(),
    prior = c(set_prior("normal(0,5)", class = "b"),
              set_prior("cauchy(0,2)", class = "sd"),
              set_prior("lkj(2)", class = "cor")),
    warmup = 1000, iter = 2000, chains = 4,
    control = list(adapt_delta = 0.95))
```

## `loo`

> Estimate model accuracy and model comparison

* Leave-one-out cross-validation (LOO) 
* Widely applicable information criterion (WAIC)
* Vehtari, A., Gelman, A., and Gabry, J. (2016a). Practical Bayesian model evaluation using leave-one-out cross-validation and WAIC. Statistics and Computing. Advance online publication. doi:10.1007/s11222-016-9696-4

# Coin example - Bernoulli model

```{r}
rm(list = ls()) ; gc()
```

## Example

5 coin throw

```{r}
y <- c(0, 1, 1, 0, 1) # Observations
```

$\theta$ approximation:

```{r}
sum(y) / length(y)
```


## Back to maths

$$p(y|\theta)=\prod_{n=1}^N \theta^{y_n}*(1-\theta)^{1-y_n}$$
$$log(p(y|\theta))=\sum_{n=1}^N{y_n}*log(\theta) + \sum_{n=1}^N(1-y_n)*log(1-\theta)$$

## "Roulé sous les aisselles" - Likelihood

$$p(y|\theta)=$$

```{r}
log_likelihood <- function(theta, y) 
  sum(log(theta)*y + log(1-theta)*(1 -y))

log_likelihood_dbinom <- function(theta, y) 
  sum(dbinom(y, size = 1, prob = theta, log = T))

exp(log_likelihood(0.1, y))
```

## "Roulé sous les aisselles" - Posterior sampling

```{r}
# theta grid
theta <- seq(0.001, 0.999, length.out = 250)

# p(theta|y)
log_posterior <- sapply(theta, function(x) log_likelihood(x, y))
posterior <- exp(log_posterior)

# posterior sampling
posterior_draws <- sample(theta, size = 1e5, 
                          replace = TRUE, prob = posterior)
```


## "Roulé sous les aisselles" - Posterior plot

```{r, echo=FALSE}
data.frame(Theta = posterior_draws) %>% 
  ggplot(aes(Theta)) + 
  geom_density() +
  geom_vline(xintercept = sum(y)/length(y), col = 'red')
```

## `stan` - Model

$$y \sim \mathcal{B}(\theta)$$

## `stan` - Model

$$y \sim \mathcal{B}(\theta)$$


```{stan output.var="", eval=F, cache=F}
data {
  int<lower=1>  N; // Number of observations
  int<lower=0, upper=1> y[N]; // Observations
}
parameters {
  real<lower=0, upper=1> theta; // Parameter
}
model {
  theta ~ uniform(0, 1); // theta prior
  y ~ bernoulli(theta); // Likelihood
} // empty line at the end (C++)

```

## `stan` - Sampling

```{r, message=FALSE, warning=FALSE}
data <- list(
  N = length(y), # Number of observation
  y = y # Observations
)

fit <- stan("./stan_models/coin_model.stan", data = data)
```

## `stan` - Sampling

```{r}
fit
```

## `stan` - Posterior

```{r}
mcmc_areas(as.array(fit), pars = "theta", prob = 0.8)
```

# Leaf lifespan example - Linear regression

```{r}
rm(list = ls()) ; gc()
```

## Glopnet data

```{r}
LES <- read.csv("./stan_data/GLOPNET.csv", skip = 10) %>% 
  filter(BIOME == "TROP_RF", GF == "T") %>%
  select(Dataset, Species, log.LL, log.LMA) %>% 
  na.omit()

kable(LES[1:5,])
```

## Glopnet data

```{r, fig.height=4}
LES %>% 
  ggplot(aes(log.LMA, log.LL, col = Dataset)) +
  geom_point() +
  xlab("logarithm of Leaf Mass per Area (LMA)") +
  ylab("logarithm of Leaf Lifespan (LL)")
```

## Model

$$log.LL \sim \mathcal{N}(\alpha + \beta*log.LMA, \sigma) $$

## Model

$$log.LL \sim \mathcal{N}(\alpha + \beta*log.LMA, \sigma) $$

```{stan output.var="", eval=F, cache=F}
data {
  int<lower=1>  N; // Number of observations
  vector<lower=0>[N] logLMA; // Leaf Mass per Area
  vector<lower=0>[N] logLL; // Leaf Lifespan
}
parameters {
  real alpha; // intercept
  real beta; // LMA parameter
  real<lower=0, upper=10> sigma; // variance
}
model {
  alpha ~ gamma(10^-2, 10^-2); // alpha prior
  beta ~ gamma(10^-2, 10^-2); // beta prior
  sigma ~ uniform(0, 10); // sigma prior
  logLL ~ normal(alpha + beta*logLMA, sigma); // Likelihood
} // empty line at the end (C++)

```

## Sampling

```{r, message=FALSE, warning=FALSE}
data <- list(
  N = dim(LES)[1],
  logLMA = LES$log.LMA,
  logLL = LES$log.LL
)

fit1 <- stan("./stan_models/LL_model.stan", data = data)
```

## Sampling

```{r}
fit1
```

## Trace of chains

```{r}
mcmc_trace(as.array(fit1),
           facet_args = list(labeller = label_parsed))
```

## Pairs

```{r}
mcmc_pairs(as.array(fit1), pars = c("alpha", "beta", "sigma"))
```

## Model 2

$$log.LL \sim \mathcal{N}(\beta*log.LMA, \sigma) $$

## Model 2

$$log.LL \sim \mathcal{N}(\beta*log.LMA, \sigma) $$

```{stan output.var="", eval=F, cache=F}
data {
  int<lower=1>  N; // Number of observations
  vector<lower=0>[N] logLMA; // Leaf Mass per Area
  vector<lower=0>[N] logLL; // Leaf Lifespan
}
parameters {
  real beta; // LMA parameter
  real<lower=0, upper=10> sigma; // variance
}
model {
  beta ~ gamma(10^-2, 10^-2); // beta prior
  logLL ~ normal(beta*logLMA, sigma); // Likelihood
} // empty line at the end (C++)

```

## Sampling 2

```{r, message=FALSE, warning=FALSE}
fit2 <- stan("./stan_models/LL2_model.stan", data = data)
```

## Sampling 2

```{r}
fit2
```

## Trace of chains 2

```{r}
mcmc_trace(as.array(fit2),
           facet_args = list(labeller = label_parsed))
```

## Pairs 2

```{r}
mcmc_pairs(as.array(fit2), pars = c("beta", "sigma"))
```

## Posteriors 2

```{r}
mcmc_areas(as.array(fit2), prob = 0.8,
           pars = c("beta", "sigma"))
```

## Predictions 2

```{r}
pars <- c("beta", "sigma")
pars_opt <- as.matrix(fit2)[which.max(as.matrix(fit2)[,'lp__']), pars]
pars_fit <- broom::tidy(fit2) %>% select(estimate) %>% unlist()
predict <- function(logLMA, pars_val)
  rlnorm(1000, meanlog = pars_val[1]*logLMA, sdlog = pars_val[2])
predictions_all <- sapply(seq_len(data$N), function(i)
  apply(as.matrix(fit2)[,pars], 1, function(p)
    predict(LES$log.LMA[i], p)))
predict_null_var <- function(logLMA, pars_val)
  rlnorm(1000, meanlog = pars_val[1]*logLMA, sdlog = 0)
predictions_all_null_var <- sapply(seq_len(data$N), function(i)
  apply(as.matrix(fit2)[,pars], 1, function(p)
    predict_null_var(LES$log.LMA[i], p)))
predictions_mean <- sapply(seq_len(data$N), function(i)
  apply(as.matrix(fit2)[,pars], 1, function(p) exp(p[1]*LES$log.LMA[i])))
predictions <- data.frame(t(apply(predictions_all, 2, function(x)
  quantile(x, probs = c(0.05, 0.95)))))
predictions$mean_predictions_var <- apply(predictions_all, 2, mean)
predictions$mean_predictions_null_var <- apply(predictions_all_null_var, 2, mean)
predictions$mean_predictions_mean <- apply(predictions_mean, 2, mean)
predictions$mean_opt <- exp(pars_opt["beta"]*LES$log.LMA)
predictions$mean_fit <- exp(pars_fit["estimate1"]*LES$log.LMA + pars_fit["estimate2"]^2/2)
predictions$logLL <- LES$log.LL
predictions$logLMA <- LES$log.LMA
```

## Predictions 2

```{r, fig.height=4}
ggplot(predictions, aes(logLMA, exp(logLL))) +
  geom_ribbon(aes(ymin = X5., ymax = X95.),
              color = "grey", alpha = 0.2) +
  geom_line(aes(y = mean_predictions), col = 'blue') +
  # geom_line(aes(y = mean_predictions_mean), col = 'black') +
  geom_line(aes(y = mean_predictions_null_var), col = 'red') +
  geom_line(aes(y = mean_fit), col = 'green') +
  # geom_line(aes(y = mean_opt), col = 'green') +
  geom_point() +
  stat_function(fun = function(lma) exp(pars_fit["estimate1"]*lma + pars_fit["estimate2"]^2/2), col = "blue")

ggplot(data.frame(LMA = 1:6), aes(x = LMA)) +
  stat_function(fun = function(lma) exp(pars_fit["estimate1"]*lma + pars_fit["estimate2"]^2/2), col = "blue")
```

## `shinystan`

```{r, eval=FALSE}
launch_shinystan(fit2)
```

## `rstanarm`

```{r}
fit_arm <- stan_lm(log.LL ~ log.LMA, data = LES, prior = NULL)
```

## `rstanarm`

```{r}
fit_arm
```

## `brms`

```{r}
fit_brms <- brm(formula = log.LL ~ log.LMA|Dataset, data = LES)
```

## `brms`

```{r}
fit_brms
```

# Growth example

```{r}
rm(list = ls()) ; gc()
```

## Data

```{r, fig.height=4}
trees <- read.csv2("./stan_data/Angelique_logaccr.csv", header = T, dec = ".")
data <- list(
  N = dim(trees)[1], # Nb of trees
  growth = trees$Y, # growth vector
  dbh = trees$X # dbh vector
)
ggplot(trees, aes(X, Y)) + geom_point()
```

## Model

$$log(growth+1) \sim \mathcal{N}(G_{max}*exp(-\frac{1}{2}(log(\frac{dbh}{D_{opt}})/K_s)^2),\sigma)$$

## Model

$$log(growth+1) \sim \mathcal{N}(G_{max}*exp(-\frac{1}{2}(log(\frac{dbh}{D_{opt}})/K_s)^2),\sigma)$$

```{stan output.var="", eval=F, cache=F}
data {
  int<lower=0> N ; // Nb of trees
  vector<lower=0>[N] growth ; // growth vector
  vector<lower=0>[N] dbh ; // dbh vector
}
parameters {
  real Gmax ; // potential growth
  real<lower=0,upper=200> Dopt ; // optimal diameter
  real Ks ; // kurtosis
  real<lower=0,upper=10> sigma ;
}
model {
  for(n in 1:N)
    log(growth[n]+1) ~ normal(Gmax*exp(-0.5*pow(log(dbh[n]/Dopt)/Ks,2)), sigma) ;
}
```

## Sampling

```{r, message=FALSE, warning=FALSE}
inits <- function() list(Gmax = 1, Dopt = 100, Ks = 0.1, sigma = 2)
fit <- stan("./stan_models/growth_model.stan", data = data,
            init = inits)
```

## Sampling

```{r}
pars <- c("Gmax", "Dopt", "Ks", "sigma")
print(fit, pars = c(pars, "lp__"))
```

## Trace of chains

```{r}
mcmc_trace(as.array(fit), pars = c(pars, "lp__"),
           facet_args = list(labeller = label_parsed))
```

## Pairs

```{r}
mcmc_pairs(as.array(fit), pars = pars)
```

## Posteriors

```{r}
mcmc_areas(as.array(fit), prob = 0.8,
           pars = pars)
```

## Predictions

```{r, fig.height=4}
pars_opt <- as.matrix(fit)[which.max(as.matrix(fit)[,'lp__']), pars]
predict <- function(dbh) 
  pars_opt["Gmax"]*exp(-0.5*(log(dbh/pars_opt["Dopt"])/pars_opt["Ks"])^2)
ggplot(trees, aes(X, log(Y+1))) +
  geom_point() +
  geom_line(aes(y = predict(trees$X)), col = "red")
```

# Gaps & growth example

```{r}
rm(list = ls()) ; gc()
```

## Growth

```{r}
source("./stan_data/dbh_correction.R")
trees <- read.csv("./stan_data/Symphonia_Paracou.csv", dec = ',') %>% 
  filter(n_parcelle == 2) %>% 
  mutate(dbh = circonf/pi) %>%
  arrange(campagne) %>% 
  group_by(idArbre) %>% 
  mutate(dbh_c = correction(dbh, campagne, code_vivant, code_mesure)) %>% 
  filter(any(campagne == 1988)) %>% 
  filter(any(campagne == 1992)) %>% 
  filter(campagne %in% c(1988, 1992)) %>% 
  mutate(growth = diff(dbh_c)) %>% 
  filter(growth > 0) %>% 
  filter(campagne == 1988) %>% 
  dplyr::select(idArbre, Xutm, Yutm, dbh_c, growth)
kable(trees[1:3,])
```

## Gaps

```{r data}
crs <- '+proj=longlat +datum=NAD83 +no_defs +ellps=GRS80 +towgs84=0,0,0'
gaps <- shapefile("./stan_data/gaps/Gaps.shp")
gaps <- subset(gaps, Plot == 2)
gaps <- spTransform(gaps, CRSobj = crs)
gaps$area <- area(gaps)
treesXY <- trees
coordinates(treesXY) <- ~Xutm + Yutm
proj4string(treesXY) <- '+proj=utm +zone=22 +datum=WGS84 +units=m +no_defs +ellps=WGS84 +towgs84=0,0,0'
treesXY <- spTransform(treesXY, CRSobj = crs)
D <- rgeos::gDistance(spTransform(gaps, "+proj=utm +zone=22 +units=m"),
                      spTransform(treesXY, "+proj=utm +zone=22 +units=m"),
                      byid = T)
```

## Data vizualisation

```{r}
d <- raster(extent(treesXY), resolution = 10^-4, crs = crs)
d <- setValues(d, 0)
d <-  mask(d, gaps)
d <- distance(d)
trees$dgaps <- raster::extract(d, treesXY)
map <- leaflet() %>%
  addRasterImage(log(d+1),
                 opacity = 0.5) %>% 
  addCircles(data = treesXY, radius = ~growth, 
             label = paste("+", round(treesXY$growth,1), "cm"))
```

## Data vizualisation

```{r}
map
```

## Data preparation

```{r}
data <- list(I = dim(trees)[1], # Nb of trees
             growth = trees$growth, # growth vector
             dbh = trees$dbh_c/max(trees$dbh_c), # dbh in 1988 vector
             J = dim(gaps)[1], # Nb of gaps
             S = gaps$area/max(gaps$area), # gaps surface vector
             D = D/max(D)) #tree-gaps distance matrix
```

## Model

$$growth_i \sim \mathcal{N}(\mu*\sum_j^J(e^{-\alpha*d_{i,j}}*S_j^\beta);\sigma)$$

## Model

$$growth_i \sim \mathcal{N}(\mu*\sum_j^J(e^{-\alpha*d_{i,j}}*S_j^\beta);\sigma)$$

```{stan output.var="", eval=F, cache=F}
data {
  int<lower=0> I ; // Nb of trees
  vector<lower=0>[I] growth ; // growth vector
  vector<lower=0>[I] dbh ; // dbh in 1988 vector
  int<lower=0> J ; // Nb of gaps
  vector<lower=0>[J] S ; // gaps surface vector
  matrix<lower=0>[I,J] D ; // tree-gaps distance matrix
}
parameters {
  real mu ;
  real alpha ; // distance parameter
  real<lower=0,upper=3> beta ; // surface parameter (power)
  real<lower=0,upper=10> sigma ;
}
transformed parameters {
  vector[J] Sbeta ; // Sbeta is S^beta because pow(vector,real) is impossible in stan
  vector[I] Idisturb ; // disturbance index
  for(j in 1:J)
    Sbeta[j] = pow(S[j], beta) ;
  for(i in 1:I)
    Idisturb[i] = exp(-alpha*D[i,])*Sbeta ;
}
model {
  growth ~ normal(mu*Idisturb, sigma) ;
}
```

## Sampling

```{r, message=FALSE, warning=FALSE}
fit <- stan("./stan_models/gap_model.stan", data = data)
```

## Sampling

```{r}
pars <- c("mu", "alpha", "beta", "sigma")
print(fit, pars = pars)
```

## Trace of chains

```{r}
mcmc_trace(as.array(fit), pars = c(pars, "lp__"),
           facet_args = list(labeller = label_parsed))
```

## Pairs

```{r}
mcmc_pairs(as.array(fit), pars = pars)
```

## Posteriors

```{r}
mcmc_areas(as.array(fit), prob = 0.8,
           pars = pars)
```

# What about `greta`

## `greta` vs. `stan`

* All in R 
    * no new language learning
    * functions writting
* Tensor-flow algorithm
* Plot models
* Support matricial equations
* Works with `bayesplot`
* Only Hamiltonian Monte Carlo (HMC) algorithm
* Younger and smaller (less support)

## Model

```{r}
# data
growth <- as_data(trees$growth)
dbh <- as_data(trees$dbh_c/max(trees$dbh_c))
S <- as_data(gaps$area/max(gaps$area))
Dm <- as_data(D/max(D))
# variables and priors
mu <- gamma(10^-2,10^-2)
alpha <- gamma(10^-2,10^-2)
beta <- gamma(10^-2,10^-2, truncation = c(0,3))
sigma <- gamma(10^-2,10^-2, truncation = c(0,10))
# operations
Idisturb <- exp(-alpha*D) %*% S^beta
# likelihood
distribution(growth) <- normal(mu*Idisturb, sigma)
# defining the model
m <- model(mu, alpha, beta, sigma)
```

## Model graph

```{r}
plot(m)
include_graphics("./stan_data/gap_model_graph.png")
```

## Sampling

```{r}
fit <- mcmc(m, n_samples = 1000)
summary(fit)
```

# Conclusion

## Model choice

* Prior, laws, and model forms knowledge
* Fitting techniques and tricks
    * center, reduce, bound, link...
* **Try and compare**
    * convergence, parameters number, likelihood, prediction quality
    * e.g. $\hat{R}$, $K$, $log(\mathcal{L})$, $RMSEP$...
    * [rotten model example](./stan_data/rotten2.html)

## References

* **R help (?)**
* Largely inspired by Daniel Furr and Ben Goodrich [presentaion](http://mc-stan.org/workshops/IMPS2016/half1.html#2) and Eric Novik presentation
* [Michael Clark blog](http://m-clark.github.io/workshops/bayesian/index.html#home) *Become a bayesian with R & stan*
* [`stan` website](mc-stan.org/)
* [`stan` GitHub](https://github.com/stan-dev) with packages and wiki
* [`stan` forum](http://discourse.mc-stan.org/)
* [`greta` website](https://greta-dev.github.io/greta/index.html)
